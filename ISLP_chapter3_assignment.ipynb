{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lubaochuan/ml_python/blob/main/ISLP_chapter3_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d55a922",
      "metadata": {
        "id": "7d55a922"
      },
      "source": [
        "# Understanding Linear Regression\n",
        "\n",
        "This guided lab builds intuition for linear regression through **hands-on experiments**.\n",
        "You will explore:\n",
        "- Model fitting and interpretation\n",
        "- RÂ² and model evaluation\n",
        "- Train/test splits and generalization\n",
        "- Biasâ€“variance intuition with interactive sliders\n",
        "- Failure cases: nonlinearity and outliers\n",
        "\n",
        "ðŸ‘‰**Sudent name:**\n",
        "\n",
        "ðŸ‘‰**Date:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a11a314",
      "metadata": {
        "id": "4a11a314"
      },
      "source": [
        "## Step 1: Setup and Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2241d1d",
      "metadata": {
        "id": "d2241d1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ipywidgets import interact, IntSlider\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.linspace(0, 10, 40)\n",
        "y = 3 * X + 5 + np.random.normal(0, 3, size=len(X))\n",
        "\n",
        "X = X.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d79ee8",
      "metadata": {
        "id": "c5d79ee8"
      },
      "source": [
        "## Step 2: Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5b0347",
      "metadata": {
        "id": "5b5b0347"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.scatter(X, y)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Observed Data\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e7919f",
      "metadata": {
        "id": "29e7919f"
      },
      "source": [
        "## Step 3: Fit a Simple Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7519772f",
      "metadata": {
        "id": "7519772f"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Slope:\", model.coef_[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d975ef",
      "metadata": {
        "id": "d8d975ef"
      },
      "source": [
        "## Step 4: RÂ² (Goodness of Fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a51225",
      "metadata": {
        "id": "39a51225"
      },
      "outputs": [],
      "source": [
        "\n",
        "r2 = r2_score(y, y_pred)\n",
        "print(\"RÂ² score:\", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ebdac7",
      "metadata": {
        "id": "d8ebdac7"
      },
      "source": [
        "\n",
        "**Interpretation of RÂ²:**\n",
        "- Measures how much of the variation in y is explained by the model\n",
        "- RÂ² = 1 means perfect explanation\n",
        "- High RÂ² does NOT guarantee good predictions on new data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5421ed3",
      "metadata": {
        "id": "a5421ed3"
      },
      "source": [
        "## Step 5: Train/Test Split (Generalization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ccb2672",
      "metadata": {
        "id": "3ccb2672"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "model_tt = LinearRegression()\n",
        "model_tt.fit(X_train, y_train)\n",
        "\n",
        "train_r2 = r2_score(y_train, model_tt.predict(X_train))\n",
        "test_r2 = r2_score(y_test, model_tt.predict(X_test))\n",
        "\n",
        "print(\"Training RÂ²:\", train_r2)\n",
        "print(\"Test RÂ²:\", test_r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c425e14c",
      "metadata": {
        "id": "c425e14c"
      },
      "source": [
        "\n",
        "**Questions:**\n",
        "1. Why is test RÂ² usually lower?\n",
        "2. What does this say about generalization?\n",
        "\n",
        "ðŸ‘‰**Your answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6dd6ccc",
      "metadata": {
        "id": "a6dd6ccc"
      },
      "source": [
        "## Step 6: Biasâ€“Variance Intuition with Model Complexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dff6a146",
      "metadata": {
        "id": "dff6a146"
      },
      "outputs": [],
      "source": [
        "\n",
        "def polynomial_fit(degree):\n",
        "    coeffs = np.polyfit(X.flatten(), y, degree)\n",
        "    y_hat = np.polyval(coeffs, X)\n",
        "\n",
        "    plt.scatter(X, y, label=\"Data\")\n",
        "    plt.plot(X, y_hat, label=f\"Degree {degree}\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Biasâ€“Variance Tradeoff\")\n",
        "    plt.show()\n",
        "\n",
        "interact(polynomial_fit, degree=IntSlider(min=1, max=10, step=1, value=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feaee2c2",
      "metadata": {
        "id": "feaee2c2"
      },
      "source": [
        "\n",
        "**Interpretation:**\n",
        "- Low degree â†’ high bias (underfitting)\n",
        "- High degree â†’ high variance (overfitting)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7274863",
      "metadata": {
        "id": "b7274863"
      },
      "source": [
        "## Step 7: Failure Case â€“ Nonlinear Relationship"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6326f0bc",
      "metadata": {
        "id": "6326f0bc"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_nl = np.linspace(0, 10, 40)\n",
        "y_nl = X_nl**2 + np.random.normal(0, 5, size=len(X_nl))\n",
        "\n",
        "X_nl = X_nl.reshape(-1, 1)\n",
        "\n",
        "model_nl = LinearRegression()\n",
        "model_nl.fit(X_nl, y_nl)\n",
        "\n",
        "plt.scatter(X_nl, y_nl)\n",
        "plt.plot(X_nl, model_nl.predict(X_nl))\n",
        "plt.title(\"Linear Model on Nonlinear Data\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a7e5872",
      "metadata": {
        "id": "8a7e5872"
      },
      "source": [
        "\n",
        "**Key idea:** Linear models struggle when the true relationship is nonlinear.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88b4f020",
      "metadata": {
        "id": "88b4f020"
      },
      "source": [
        "## Step 8: Failure Case â€“ Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14af1548",
      "metadata": {
        "id": "14af1548"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_out = X.copy()\n",
        "y_out = y.copy()\n",
        "\n",
        "# Add extreme outliers\n",
        "y_out[0] += 30\n",
        "y_out[-1] -= 25\n",
        "\n",
        "model_out = LinearRegression()\n",
        "model_out.fit(X_out, y_out)\n",
        "\n",
        "plt.scatter(X_out, y_out)\n",
        "plt.plot(X_out, model_out.predict(X_out))\n",
        "plt.title(\"Effect of Outliers on Linear Regression\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda411f9",
      "metadata": {
        "id": "dda411f9"
      },
      "source": [
        "\n",
        "**Key idea:** Least squares is sensitive to extreme outliers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c41e55",
      "metadata": {
        "id": "98c41e55"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b926f738",
      "metadata": {
        "id": "b926f738"
      },
      "source": [
        "\n",
        "1. What does RÂ² measure, and what does it not measure?\n",
        "2. Why is test performance more important than training performance?\n",
        "3. How does model complexity relate to bias and variance?\n",
        "4. Why does linear regression fail on nonlinear data?\n",
        "5. Why are outliers especially problematic for linear regression?\n",
        "\n",
        "ðŸ‘‰**Your answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c6d081c",
      "metadata": {
        "id": "4c6d081c"
      },
      "source": [
        "## Key Takeaways"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69062cbc",
      "metadata": {
        "id": "69062cbc"
      },
      "source": [
        "\n",
        "- Linear regression is powerful but limited\n",
        "- RÂ² must be interpreted carefully\n",
        "- Generalization matters more than fit\n",
        "- Biasâ€“variance tradeoff guides model choice\n",
        "- Understanding failure cases is essential\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
