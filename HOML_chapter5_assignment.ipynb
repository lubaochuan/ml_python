{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lubaochuan/ml_python/blob/main/HOML_chapter5_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d7109c3",
      "metadata": {
        "id": "8d7109c3"
      },
      "source": [
        "# Chapter 5: Support Vector Machines (SVMs)\n",
        "\n",
        "**Textbook:** *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (3rd ed.)*  \n",
        "\n",
        "## goals\n",
        "\n",
        "- Explain what an SVM is trying to do (maximize the margin).\n",
        "- Train **Linear SVM** classifiers and understand **soft margin** (the `C` hyperparameter).\n",
        "- Use **kernel SVMs** (RBF and polynomial) and interpret `gamma` and `C`.\n",
        "\n",
        "**Name:**\n",
        "\n",
        "**Date:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "097155e7",
      "metadata": {
        "id": "097155e7"
      },
      "source": [
        "## 0) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0947360d",
      "metadata": {
        "id": "0947360d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"Pandas:\", pd.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc5e12a2",
      "metadata": {
        "id": "bc5e12a2"
      },
      "source": [
        "## 1) Explore a dataset (start simple): Iris (2 classes, 2 features)\n",
        "\n",
        "We begin with the classic **Iris** dataset, but we'll restrict it to:\n",
        "- **Two classes** (e.g., Setosa vs Versicolor), and\n",
        "- **Two features** so we can visualize decision boundaries.\n",
        "\n",
        "**Why start here?**  \n",
        "SVMs are easiest to understand visually: they find a **decision boundary** with a **maximum margin**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e3b5761"
      },
      "source": [
        "**Annotation:**\n",
        "\n",
        "The Iris dataset is a classic and very popular dataset in machine learning and statistics. It contains 150 samples of iris flowers, with 50 samples from each of three species: Iris setosa, Iris versicolor, and Iris virginica.\n",
        "\n",
        "For each sample, four features are measured in centimeters:\n",
        "*   **Sepal length**\n",
        "*   **Sepal width**\n",
        "*   **Petal length**\n",
        "*   **Petal width**\n",
        "\n",
        "The goal of using this dataset is often to classify the species of the iris flower based on these four features. It's frequently used for teaching and demonstrating classification algorithms."
      ],
      "id": "6e3b5761"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacae59b",
      "metadata": {
        "id": "cacae59b"
      },
      "outputs": [],
      "source": [
        "iris = load_iris(as_frame=True)\n",
        "df = iris.frame.copy()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b41e3b",
      "metadata": {
        "id": "92b41e3b"
      },
      "outputs": [],
      "source": [
        "# Two classes: 0 (setosa) and 1 (versicolor)\n",
        "df_bin = df[df[\"target\"].isin([0, 1])].copy()\n",
        "\n",
        "# Two features for easy plotting\n",
        "features = [\"sepal length (cm)\", \"sepal width (cm)\"]\n",
        "X = df_bin[features].to_numpy()\n",
        "y = df_bin[\"target\"].to_numpy()\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"Class counts:\", pd.Series(y).value_counts().to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a74edcf"
      },
      "source": [
        "**Annotation:**\n",
        "\n",
        "This code snippet first filters the Iris dataset to include only two specific classes (setosa and versicolor) from the 'target' column, creating a new DataFrame called `df_bin`. It then selects two features, 'sepal length (cm)' and 'sepal width (cm)', to be used as input data, storing them in the NumPy array `X`. The corresponding target labels (0 or 1) are stored in the NumPy array `y`. Finally, it prints the shape of `X` (which tells you how many samples and features are being used) and the number of instances for each class in `y`."
      ],
      "id": "2a74edcf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53854e3f",
      "metadata": {
        "id": "53854e3f"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.xlabel(features[0])\n",
        "plt.ylabel(features[1])\n",
        "plt.title(\"Iris (2 classes) — raw feature space\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64daf72c",
      "metadata": {
        "id": "64daf72c"
      },
      "source": [
        "### Review Questions\n",
        "\n",
        "1. What does each point represent in the scatter plot?\n",
        "2. Why did we restrict to two classes and two features for this first experiment?\n",
        "3. In your own words, what does a **classification boundary** mean?\n",
        "\n",
        "**Your answers:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21725db8",
      "metadata": {
        "id": "21725db8"
      },
      "source": [
        "## 2) Train a Linear SVM\n",
        "\n",
        "We'll use `LinearSVC` for linear classification. It is fast for many problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22833b68",
      "metadata": {
        "id": "22833b68"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Linear SVM WITHOUT scaling (not recommended)\n",
        "lin_no_scale = LinearSVC(C=1.0, random_state=42, max_iter=50_000)\n",
        "lin_no_scale.fit(X_train, y_train)\n",
        "\n",
        "pred_no_scale = lin_no_scale.predict(X_test)\n",
        "print(f\"Accuracy (no scaling): {accuracy_score(y_test, pred_no_scale)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2033080",
      "metadata": {
        "id": "a2033080"
      },
      "source": [
        "## 3) Soft margin and the role of **C**\n",
        "\n",
        "For linear SVMs:\n",
        "- **Small `C`** → more regularization → wider margin, more violations allowed.\n",
        "- **Large `C`** → less regularization → tries to classify training points correctly, narrower margin.\n",
        "\n",
        "We'll compare a few values of `C` and see how the training/test accuracy changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "521527c0",
      "metadata": {
        "id": "521527c0"
      },
      "outputs": [],
      "source": [
        "Cs = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "results = []\n",
        "\n",
        "for C in Cs:\n",
        "    model = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm\", LinearSVC(C=C, random_state=42, max_iter=50_000))\n",
        "    ])\n",
        "    model.fit(X_train, y_train)\n",
        "    train_acc = model.score(X_train, y_train)\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "    results.append((C, train_acc, test_acc))\n",
        "\n",
        "pd.DataFrame(results, columns=[\"C\", \"train_accuracy\", \"test_accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2703e83f",
      "metadata": {
        "id": "2703e83f"
      },
      "source": [
        "### Review Questions\n",
        "1. As `C` increases, what typically happens to **training accuracy**?\n",
        "2. Did test accuracy always increase with larger `C`? Why might it not?\n",
        "3. In one sentence, explain the **bias–variance** intuition behind `C`.\n",
        "\n",
        "**Your answers:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4d59ab",
      "metadata": {
        "id": "ad4d59ab"
      },
      "source": [
        "## 4) When data isn't linearly separable: a nonlinear dataset (moons)\n",
        "\n",
        "Many real datasets cannot be separated by a straight line.  \n",
        "We'll generate a simple nonlinear dataset and visualize it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f1eb6e",
      "metadata": {
        "id": "49f1eb6e"
      },
      "outputs": [],
      "source": [
        "X_moons, y_moons = make_moons(n_samples=800, noise=0.25, random_state=42)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_moons[:, 0], X_moons[:, 1])\n",
        "plt.title(\"make_moons dataset (nonlinear)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n",
        "\n",
        "pd.Series(y_moons).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da84e00d",
      "metadata": {
        "id": "da84e00d"
      },
      "source": [
        "### Review Questions\n",
        "1. Why would a linear classifier struggle on this dataset?\n",
        "\n",
        "**Your answer:*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f407b3",
      "metadata": {
        "id": "e5f407b3"
      },
      "source": [
        "## 5) Kernel SVM (RBF): understanding `gamma` and `C`\n",
        "\n",
        "A **kernel SVM** can create nonlinear boundaries by implicitly mapping data into higher-dimensional spaces.\n",
        "\n",
        "For the RBF kernel:\n",
        "- `gamma` controls how \"wiggly\" the boundary can be (influence radius of points).\n",
        "- `C` controls the soft margin (regularization).\n",
        "\n",
        "We'll train a few models and compare metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "014fac6f",
      "metadata": {
        "id": "014fac6f"
      },
      "outputs": [],
      "source": [
        "Xm_train, Xm_test, ym_train, ym_test = train_test_split(\n",
        "    X_moons, y_moons, test_size=0.25, stratify=y_moons, random_state=42\n",
        ")\n",
        "\n",
        "def train_eval_svc(C, gamma):\n",
        "    model = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm\", SVC(kernel=\"rbf\", C=C, gamma=gamma, random_state=42))\n",
        "    ])\n",
        "    model.fit(Xm_train, ym_train)\n",
        "    return model.score(Xm_train, ym_train), model.score(Xm_test, ym_test)\n",
        "\n",
        "settings = [(0.1, 0.1), (1.0, 0.1), (1.0, 1.0), (10.0, 0.1), (10.0, 1.0)]\n",
        "rows = []\n",
        "for C, gamma in settings:\n",
        "    tr, te = train_eval_svc(C, gamma)\n",
        "    rows.append((C, gamma, tr, te))\n",
        "\n",
        "pd.DataFrame(rows, columns=[\"C\", \"gamma\", \"train_accuracy\", \"test_accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3691577",
      "metadata": {
        "id": "d3691577"
      },
      "source": [
        "### Review Questions\n",
        "1. What tends to happen when you increase `gamma` (keeping `C` fixed)?\n",
        "2. What tends to happen when you increase `C` (keeping `gamma` fixed)?\n",
        "3. Which combinations show signs of **overfitting**? Explain your evidence using train vs test accuracy.\n",
        "\n",
        "**Your answers:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "646cb0c8",
      "metadata": {
        "id": "646cb0c8"
      },
      "source": [
        "## 6) Chapter 5 Takeaways (Checklist)\n",
        "\n",
        "You should now be able to explain:\n",
        "\n",
        "- **Margin:** SVM tries to separate classes with the largest possible margin.  \n",
        "- **Support vectors:** The training points that influence the boundary the most.  \n",
        "- **Soft margin (`C`):** Controls the tradeoff between a wide margin and classification errors.  \n",
        "- **Scaling:** Almost always necessary for SVMs.  \n",
        "- **Kernels:** Enable nonlinear boundaries (RBF and polynomial are common).  \n",
        "- **`gamma` (RBF):** Controls locality / boundary complexity; larger = more complex.  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}