{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lubaochuan/ml_python/blob/main/ISLP_chapter4_guided_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0406d3ff",
      "metadata": {
        "id": "0406d3ff"
      },
      "source": [
        "\n",
        "# ISLP Chapter 4 – Guided Exercise: Classification, Decision Boundaries, and Probability Heatmaps\n",
        "\n",
        "This lab focuses on **intuition**, not math. You will:\n",
        "- Train **logistic regression** and **k-NN** classifiers\n",
        "- Visualize **decision boundaries**\n",
        "- Visualize **probability heatmaps** (confidence maps)\n",
        "- Compare models using **accuracy** and **confusion matrices**\n",
        "- Explore how **thresholds** change errors (false positives vs false negatives)\n",
        "\n",
        "**Tip:** Run cells top to bottom. Stop at reflection prompts and discuss with a partner.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e71d31be",
      "metadata": {
        "id": "e71d31be"
      },
      "source": [
        "## Step 0: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b363481",
      "metadata": {
        "id": "3b363481"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "np.random.seed(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7496b807",
      "metadata": {
        "id": "7496b807"
      },
      "source": [
        "## Step 1: Create a 2D dataset (two overlapping classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d20f6e",
      "metadata": {
        "id": "e0d20f6e"
      },
      "outputs": [],
      "source": [
        "\n",
        "n = 400\n",
        "X0 = np.random.multivariate_normal([0,0], [[1.3,0.5],[0.5,1.1]], n//2)\n",
        "X1 = np.random.multivariate_normal([2.2,2.0], [[1.3,-0.4],[-0.4,1.0]], n//2)\n",
        "\n",
        "X = np.vstack([X0, X1])\n",
        "y = np.array([0]*(n//2) + [1]*(n//2))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, alpha=0.8)\n",
        "plt.title(\"Training data (2 classes)\")\n",
        "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "119802d5",
      "metadata": {
        "id": "119802d5"
      },
      "source": [
        "\n",
        "### Reflection\n",
        "1. Do you expect *perfect* accuracy on test data? Why or why not?\n",
        "2. What does overlap between classes mean for the “best possible” classifier?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bb1c853",
      "metadata": {
        "id": "3bb1c853"
      },
      "source": [
        "## Helper functions: decision boundary + probability heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62085327",
      "metadata": {
        "id": "62085327"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_decision_boundary(model, X, y, title, proba=False):\n",
        "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
        "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
        "\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.linspace(x_min, x_max, 300),\n",
        "        np.linspace(y_min, y_max, 300)\n",
        "    )\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "    if proba and hasattr(model, \"predict_proba\"):\n",
        "        zz = model.predict_proba(grid)[:,1].reshape(xx.shape)\n",
        "        plt.contourf(xx, yy, zz, alpha=0.35)\n",
        "        plt.colorbar(label=\"P(class=1)\")\n",
        "        plt.contour(xx, yy, zz, levels=[0.5], colors=\"black\", linewidths=1)\n",
        "    else:\n",
        "        zz = model.predict(grid).reshape(xx.shape)\n",
        "        plt.contourf(xx, yy, zz, alpha=0.25)\n",
        "\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, edgecolor=\"k\", alpha=0.8)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13992d83",
      "metadata": {
        "id": "13992d83"
      },
      "source": [
        "## Step 2: Logistic Regression (parametric, linear boundary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ef43e1",
      "metadata": {
        "id": "f5ef43e1"
      },
      "outputs": [],
      "source": [
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "pred_lr = logreg.predict(X_test)\n",
        "acc_lr = accuracy_score(y_test, pred_lr)\n",
        "cm_lr = confusion_matrix(y_test, pred_lr)\n",
        "\n",
        "print(\"Logistic Regression accuracy:\", round(acc_lr, 3))\n",
        "print(\"Confusion matrix:\\n\", cm_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0842e082",
      "metadata": {
        "id": "0842e082"
      },
      "source": [
        "### Step 2a: Logistic regression probability heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3962447f",
      "metadata": {
        "id": "3962447f"
      },
      "outputs": [],
      "source": [
        "\n",
        "plot_decision_boundary(logreg, X_train, y_train,\n",
        "                       title=\"Logistic Regression: Probability Heatmap + 0.5 Boundary\",\n",
        "                       proba=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e51a3f60",
      "metadata": {
        "id": "e51a3f60"
      },
      "source": [
        "\n",
        "### Reflection\n",
        "1. Why is the boundary roughly a straight line?\n",
        "2. What does a “lighter” vs “darker” area mean on the heatmap?\n",
        "3. Where is the model least confident?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0c07013",
      "metadata": {
        "id": "c0c07013"
      },
      "source": [
        "## Step 3: k-NN Classification (non-parametric, flexible boundary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e44d663",
      "metadata": {
        "id": "6e44d663"
      },
      "outputs": [],
      "source": [
        "\n",
        "knn_1 = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\n",
        "knn_5 = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n",
        "knn_25 = KNeighborsClassifier(n_neighbors=25).fit(X_train, y_train)\n",
        "\n",
        "models = {\n",
        "    \"k-NN (k=1)\": knn_1,\n",
        "    \"k-NN (k=5)\": knn_5,\n",
        "    \"k-NN (k=25)\": knn_25,\n",
        "}\n",
        "for name, m in models.items():\n",
        "    pred = m.predict(X_test)\n",
        "    print(name, \"accuracy:\", round(accuracy_score(y_test, pred), 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af20190",
      "metadata": {
        "id": "2af20190"
      },
      "source": [
        "### Step 3a: Decision boundaries for k-NN (compare k values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f8656d8",
      "metadata": {
        "id": "8f8656d8"
      },
      "outputs": [],
      "source": [
        "\n",
        "for name, m in models.items():\n",
        "    plot_decision_boundary(m, X_train, y_train, title=f\"{name}: Decision Boundary\", proba=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49a3341b",
      "metadata": {
        "id": "49a3341b"
      },
      "source": [
        "### Step 3b: k-NN probability heatmaps (optional but enlightening)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "210de877",
      "metadata": {
        "id": "210de877"
      },
      "outputs": [],
      "source": [
        "\n",
        "# k-NN probabilities are local vote fractions; heatmaps show how “confident” local neighborhoods are.\n",
        "for name, m in {\"k-NN (k=5)\": knn_5, \"k-NN (k=25)\": knn_25}.items():\n",
        "    plot_decision_boundary(m, X_train, y_train, title=f\"{name}: Probability Heatmap\", proba=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6826095",
      "metadata": {
        "id": "a6826095"
      },
      "source": [
        "\n",
        "### Reflection\n",
        "1. Which k produces the most jagged boundary? Why?\n",
        "2. Which k produces the smoothest boundary? Why?\n",
        "3. Relate k to bias–variance (small k vs large k).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fd5a8e6",
      "metadata": {
        "id": "5fd5a8e6"
      },
      "source": [
        "## Step 4: Thresholding (Why probabilities matter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f420bba",
      "metadata": {
        "id": "8f420bba"
      },
      "outputs": [],
      "source": [
        "\n",
        "probs_test = logreg.predict_proba(X_test)[:,1]\n",
        "\n",
        "for t in [0.3, 0.5, 0.7]:\n",
        "    preds = (probs_test >= t).astype(int)\n",
        "    print(\"\\nThreshold =\", t)\n",
        "    print(\"Accuracy:\", round(accuracy_score(y_test, preds), 3))\n",
        "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab83bab4",
      "metadata": {
        "id": "ab83bab4"
      },
      "source": [
        "\n",
        "### Reflection\n",
        "1. As the threshold increases, do false positives increase or decrease?\n",
        "2. As the threshold increases, do false negatives increase or decrease?\n",
        "3. Give one real-world scenario where you would use a high threshold, and one where you would use a low threshold.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a42ad4c",
      "metadata": {
        "id": "2a42ad4c"
      },
      "source": [
        "\n",
        "# Review Questions\n",
        "\n",
        "1. Why is logistic regression called a **classification** method?\n",
        "2. What is a **decision boundary**?\n",
        "3. What does a probability heatmap show that a hard boundary does not?\n",
        "4. Why can **accuracy** be misleading?\n",
        "5. How does changing **k** affect k-NN behavior?\n",
        "6. How does changing the **threshold** affect error types?\n",
        "7. When might you choose logistic regression over k-NN (and vice versa)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46eda96a",
      "metadata": {
        "id": "46eda96a"
      },
      "source": [
        "<details>\n",
        "<summary>Answer Key</summary>\n",
        "\n",
        "1. It outputs P(class=1|X) and then applies a threshold to decide class labels.\n",
        "2. A curve (often a line for logistic regression) separating regions predicted as different classes.\n",
        "3. Confidence/uncertainty: which areas are “close calls” vs clearly one class.\n",
        "4. It hides *which* mistakes happen and fails under class imbalance.\n",
        "5. Small k → high variance (jagged boundary); large k → higher bias (smooth boundary).\n",
        "6. Higher threshold → fewer false positives but more false negatives (typically); lower threshold does the opposite.\n",
        "7. Choose logistic regression for interpretability, stability, small data, or when a linear boundary is reasonable; choose k-NN when you have lots of data and expect complex boundaries.\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}