{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lubaochuan/ml_python/blob/main/ISLP_chapter4_LogReg_vs_kNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf0aa6e9",
      "metadata": {
        "id": "cf0aa6e9"
      },
      "source": [
        "\n",
        "# ISLP Chapter 4 Contrast Exercise\n",
        ": Logistic Regression vs k-NN (Classification)\n",
        "\n",
        "**Goal:** Develop intuition for *when* logistic regression or k-NN is a better choice.\n",
        "\n",
        "You will compare models on three scenarios:\n",
        "1. Mostly linear boundary (logistic regression should shine)\n",
        "2. Nonlinear boundary (k-NN can shine)\n",
        "3. High-dimensional noise features (k-NN can struggle)\n",
        "\n",
        "For each scenario:\n",
        "- Train/test split\n",
        "- Accuracy + confusion matrix\n",
        "- Boundary visualization (2D scenarios)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8b82f4",
      "metadata": {
        "id": "7d8b82f4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "np.random.seed(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df37be77",
      "metadata": {
        "id": "df37be77"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_boundary(model, X, y, title):\n",
        "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
        "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
        "                         np.linspace(y_min, y_max, 300))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    zz = model.predict(grid).reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, zz, alpha=0.25)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, edgecolor=\"k\", alpha=0.8)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "    plt.show()\n",
        "\n",
        "def eval_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, pred)\n",
        "    cm = confusion_matrix(y_test, pred)\n",
        "    print(f\"{name} accuracy: {acc:.3f}\")\n",
        "    print(\"Confusion matrix:\\n\", cm)\n",
        "    return acc, cm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dedbe630",
      "metadata": {
        "id": "dedbe630"
      },
      "source": [
        "## Scenario 1: Mostly linear decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b809f22d",
      "metadata": {
        "id": "b809f22d"
      },
      "outputs": [],
      "source": [
        "\n",
        "n = 500\n",
        "X = np.random.normal(0, 1, (n, 2))\n",
        "# Linear-ish rule + noise\n",
        "y = ((1.2*X[:,0] + 0.9*X[:,1] + np.random.normal(0, 0.6, n)) > 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "knn5 = KNeighborsClassifier(5)\n",
        "\n",
        "print(\"Scenario 1\")\n",
        "eval_model(\"Logistic Regression\", logreg, X_train, X_test, y_train, y_test)\n",
        "eval_model(\"k-NN (k=5)\", knn5, X_train, X_test, y_train, y_test)\n",
        "\n",
        "plot_boundary(logreg.fit(X_train, y_train), X_train, y_train, \"LogReg boundary (Scenario 1)\")\n",
        "plot_boundary(knn5.fit(X_train, y_train), X_train, y_train, \"k-NN boundary (Scenario 1)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93222bf",
      "metadata": {
        "id": "b93222bf"
      },
      "source": [
        "\n",
        "### Reflection\n",
        "1. Which model performed better? Why might that be?\n",
        "2. Which boundary looks simpler? Which looks more flexible?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f545bff",
      "metadata": {
        "id": "7f545bff"
      },
      "source": [
        "## Scenario 2: Nonlinear decision boundary (ring vs center)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dfe6b56",
      "metadata": {
        "id": "6dfe6b56"
      },
      "outputs": [],
      "source": [
        "\n",
        "n = 600\n",
        "X = np.random.normal(0, 1.2, (n, 2))\n",
        "r = np.sqrt(X[:,0]**2 + X[:,1]**2)\n",
        "# Ring classification with noise\n",
        "y = ((r + np.random.normal(0, 0.15, n)) > 1.2).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "knn15 = KNeighborsClassifier(15)\n",
        "\n",
        "print(\"Scenario 2\")\n",
        "eval_model(\"Logistic Regression\", logreg, X_train, X_test, y_train, y_test)\n",
        "eval_model(\"k-NN (k=15)\", knn15, X_train, X_test, y_train, y_test)\n",
        "\n",
        "plot_boundary(logreg.fit(X_train, y_train), X_train, y_train, \"LogReg boundary (Scenario 2)\")\n",
        "plot_boundary(knn15.fit(X_train, y_train), X_train, y_train, \"k-NN boundary (Scenario 2)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "643197f4",
      "metadata": {
        "id": "643197f4"
      },
      "source": [
        "\n",
        "### Reflection\n",
        "1. Why does logistic regression struggle here?\n",
        "2. How is k-NN able to adapt?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eed2541",
      "metadata": {
        "id": "4eed2541"
      },
      "source": [
        "## Scenario 3: Add many irrelevant features (curse of dimensionality intuition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83920a24",
      "metadata": {
        "id": "83920a24"
      },
      "outputs": [],
      "source": [
        "\n",
        "n = 800\n",
        "# Start with 2 informative features\n",
        "X2 = np.random.normal(0, 1, (n, 2))\n",
        "y = ((1.0*X2[:,0] - 1.2*X2[:,1] + np.random.normal(0, 0.6, n)) > 0).astype(int)\n",
        "\n",
        "# Add 20 irrelevant noise features\n",
        "noise = np.random.normal(0, 1, (n, 20))\n",
        "X = np.hstack([X2, noise])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
        "\n",
        "logreg = LogisticRegression(max_iter=2000)\n",
        "knn5 = KNeighborsClassifier(5)\n",
        "\n",
        "print(\"Scenario 3 (22 features: 2 signal + 20 noise)\")\n",
        "eval_model(\"Logistic Regression\", logreg, X_train, X_test, y_train, y_test)\n",
        "eval_model(\"k-NN (k=5)\", knn5, X_train, X_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48632bf8",
      "metadata": {
        "id": "48632bf8"
      },
      "source": [
        "\n",
        "### Reflection\n",
        "1. Why might k-NN degrade when we add many irrelevant features?\n",
        "2. What preprocessing might help k-NN?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb729f1a",
      "metadata": {
        "id": "cb729f1a"
      },
      "source": [
        "\n",
        "# Questions\n",
        "\n",
        "1. When would you choose logistic regression over k-NN?\n",
        "2. When would you choose k-NN over logistic regression?\n",
        "3. How does model choice relate to interpretability, data size, and decision boundary shape?\n",
        "4. If false negatives are extremely costly, how would you change your threshold strategy?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f20279a",
      "metadata": {
        "id": "1f20279a"
      },
      "source": [
        "<details>\n",
        "<summary>Answer Key</summary>\n",
        "\n",
        "1. Choose logistic regression for interpretability, stability, probability outputs, small datasets, or when boundary is roughly linear.\n",
        "2. Choose k-NN when boundary is complex/nonlinear and you have enough data; but be cautious with high-dimensional features.\n",
        "3. LR = global linear boundary + interpretable coefficients; k-NN = local voting, flexible boundary, sensitive to scaling/dimensionality.\n",
        "4. Lower the threshold to catch more positives (increase recall), accepting more false positives.\n",
        "</details>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}