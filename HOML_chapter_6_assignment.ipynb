{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lubaochuan/ml_python/blob/main/HOML_chapter_6_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b62baf3",
      "metadata": {
        "id": "7b62baf3"
      },
      "source": [
        "Chapter 6: Decision Trees (Fundamentals)\n",
        "\n",
        "**Textbook:** *Hands-On Machine Learning (3rd ed.)*\n",
        "\n",
        "This activity focuses on intuition: splitting, impurity, overfitting, and regularization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3e01056",
      "metadata": {
        "id": "c3e01056"
      },
      "source": [
        "## 0) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47cc2608",
      "metadata": {
        "id": "47cc2608"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "np.random.seed(42)\n",
        "print('NumPy:', np.__version__)\n",
        "print('Pandas:', pd.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c745a867",
      "metadata": {
        "id": "c745a867"
      },
      "source": [
        "## 1) Explore a dataset: Iris (2 features)\n",
        "\n",
        "We use two features so we can visualize splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0da6031",
      "metadata": {
        "id": "d0da6031"
      },
      "outputs": [],
      "source": [
        "iris = load_iris(as_frame=True)\n",
        "df = iris.frame.copy()\n",
        "features = ['petal length (cm)', 'petal width (cm)']\n",
        "X = df[features].to_numpy()\n",
        "y = df['target'].to_numpy()\n",
        "print('X shape:', X.shape)\n",
        "print('Class counts:', pd.Series(y).value_counts().to_dict())\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0], X[:,1])\n",
        "plt.xlabel(features[0]); plt.ylabel(features[1])\n",
        "plt.title('Iris — two-feature view')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0df68664",
      "metadata": {
        "id": "0df68664"
      },
      "source": [
        "### Review questions\n",
        "1. What does each point represent?\n",
        "2. What does it mean for a region to be “pure”?\n",
        "\n",
        "**Your answers:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd68b21",
      "metadata": {
        "id": "0cd68b21"
      },
      "source": [
        "## 2) Train a shallow decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91983110",
      "metadata": {
        "id": "91983110"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "pred = tree_clf.predict(X_test)\n",
        "print('Test accuracy:', accuracy_score(y_test, pred))\n",
        "plt.figure(figsize=(12,6))\n",
        "plot_tree(tree_clf, feature_names=features, class_names=iris.target_names,\n",
        "          filled=True, rounded=True)\n",
        "plt.title('Decision Tree (max_depth=2)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6ea6d37",
      "metadata": {
        "id": "b6ea6d37"
      },
      "source": [
        "### Review questions\n",
        "1. Which feature is used at the root?\n",
        "2. What does `gini=0` indicate?\n",
        "\n",
        "**Your answers:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "702b6691",
      "metadata": {
        "id": "702b6691"
      },
      "source": [
        "## 3) Impurity intuition: Gini vs Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0317e9b",
      "metadata": {
        "id": "f0317e9b"
      },
      "outputs": [],
      "source": [
        "def gini(counts):\n",
        "    counts = np.array(counts, dtype=float)\n",
        "    p = counts / counts.sum()\n",
        "    return 1.0 - np.sum(p**2)\n",
        "\n",
        "def entropy(counts):\n",
        "    counts = np.array(counts, dtype=float)\n",
        "    p = counts / counts.sum()\n",
        "    p = p[p>0]\n",
        "    return -np.sum(p*np.log2(p))\n",
        "\n",
        "examples = [([10,0,0],'Pure'), ([8,2,0],'Mostly one class'), ([4,3,3],'Mixed')]\n",
        "rows=[]\n",
        "for c,name in examples:\n",
        "    rows.append((name,c, round(gini(c),3), round(entropy(c),3)))\n",
        "pd.DataFrame(rows, columns=['Node','Counts','Gini','Entropy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c7e381",
      "metadata": {
        "id": "c8c7e381"
      },
      "source": [
        "### Review questions\n",
        "1. Which mix is most impure?\n",
        "2. Why would reducing impurity help classification?\n",
        "\n",
        "**Your answers:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b54fc5",
      "metadata": {
        "id": "34b54fc5"
      },
      "source": [
        "## 4) Overfitting: deep vs shallow (Iris)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30017713",
      "metadata": {
        "id": "30017713"
      },
      "outputs": [],
      "source": [
        "deep_tree = DecisionTreeClassifier(random_state=42)\n",
        "deep_tree.fit(X_train, y_train)\n",
        "shallow_tree = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "shallow_tree.fit(X_train, y_train)\n",
        "print('Deep   train/test:', deep_tree.score(X_train,y_train), deep_tree.score(X_test,y_test))\n",
        "print('Shallow train/test:', shallow_tree.score(X_train,y_train), shallow_tree.score(X_test,y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3deeaef",
      "metadata": {
        "id": "c3deeaef"
      },
      "source": [
        "## 5) Boxy regions on nonlinear data (make_moons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eb30a8c",
      "metadata": {
        "id": "2eb30a8c"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "X_m, y_m = make_moons(n_samples=800, noise=0.25, random_state=42)\n",
        "Xm_train, Xm_test, ym_train, ym_test = train_test_split(\n",
        "    X_m, y_m, test_size=0.25, stratify=y_m, random_state=42)\n",
        "tree_m = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_m.fit(Xm_train, ym_train)\n",
        "print('Train acc:', tree_m.score(Xm_train, ym_train))\n",
        "print('Test acc:', tree_m.score(Xm_test, ym_test))\n",
        "def plot_decision_boundary(model, X, title):\n",
        "    x0_min,x0_max = X[:,0].min()-0.5, X[:,0].max()+0.5\n",
        "    x1_min,x1_max = X[:,1].min()-0.5, X[:,1].max()+0.5\n",
        "    xx0,xx1 = np.meshgrid(np.linspace(x0_min,x0_max,300), np.linspace(x1_min,x1_max,300))\n",
        "    grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
        "    Z = model.predict(grid).reshape(xx0.shape)\n",
        "    plt.figure()\n",
        "    plt.contourf(xx0, xx1, Z, alpha=0.25)\n",
        "    plt.scatter(X[:,0], X[:,1])\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
        "    plt.show()\n",
        "plot_decision_boundary(tree_m, Xm_test, 'Decision Tree boundary (max_depth=3)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "287e60e4",
      "metadata": {
        "id": "287e60e4"
      },
      "source": [
        "## 6) Regularization sweep: max_depth\n",
        "\n",
        "Observe train vs test accuracy as depth increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ce06952",
      "metadata": {
        "id": "9ce06952"
      },
      "outputs": [],
      "source": [
        "depths = list(range(1, 16))\n",
        "train_scores=[]; test_scores=[]\n",
        "for d in depths:\n",
        "    m = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
        "    m.fit(Xm_train, ym_train)\n",
        "    train_scores.append(m.score(Xm_train, ym_train))\n",
        "    test_scores.append(m.score(Xm_test, ym_test))\n",
        "plt.figure()\n",
        "plt.plot(depths, train_scores, label='train')\n",
        "plt.plot(depths, test_scores, label='test')\n",
        "plt.xlabel('max_depth'); plt.ylabel('Accuracy')\n",
        "plt.title('Train vs Test vs Depth')\n",
        "plt.legend(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba257e93",
      "metadata": {
        "id": "ba257e93"
      },
      "source": [
        "## 7) Feature importance (Breast Cancer)\n",
        "\n",
        "Importances show which features reduced impurity the most (not causation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be486900",
      "metadata": {
        "id": "be486900"
      },
      "outputs": [],
      "source": [
        "bc = load_breast_cancer(as_frame=True)\n",
        "X_bc = bc.data\n",
        "y_bc = bc.target\n",
        "Xb_train, Xb_test, yb_train, yb_test = train_test_split(\n",
        "    X_bc, y_bc, test_size=0.2, stratify=y_bc, random_state=42)\n",
        "tree_bc = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "tree_bc.fit(Xb_train, yb_train)\n",
        "print('Train acc:', tree_bc.score(Xb_train, yb_train))\n",
        "print('Test acc:', tree_bc.score(Xb_test, yb_test))\n",
        "importances = pd.Series(tree_bc.feature_importances_, index=X_bc.columns).sort_values(ascending=False)\n",
        "importances.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5ede6f2",
      "metadata": {
        "id": "c5ede6f2"
      },
      "source": [
        "## Takeaways & reflection\n",
        "1. Name two hyperparameters to reduce overfitting.\n",
        "2. Why do trees tend to create boxy boundaries?\n",
        "3. Why can feature importance be misleading?\n",
        "\n",
        "**Your answers:**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}